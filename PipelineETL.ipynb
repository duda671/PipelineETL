{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeUhByOOWwkY"
      },
      "source": [
        "# Pipeline ETL de dados da Web utilizando Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7SBG5PkW3Qb"
      },
      "source": [
        "Um pipeline ETL de dados é um conjunto de processos organizados e automatizados para extrair, transformar e carregar (ETL) dados de várias fontes para um destino específico, geralmente um armazenamento de dados ou um data warehouse. Esses pipelines são amplamente utilizados para movimentar e preparar dados, os preparando para análises, relatórios e outras aplicações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA3N-VnAXKat"
      },
      "source": [
        "\n",
        "1.   **<font color=\"#f4a261\">Extração (Extract):</font>** Nesta etapa, os dados são coletados de diversas fontes, como bancos de dados, planilhas, APIs, arquivos CSV, entre outros. A extração pode envolver a leitura direta dos dados ou a replicação de dados de sistemas de origem para o sistema de destino.\n",
        "2.  **<font color=\"#f4a261\"> Transformação (Transform): </font>** Os dados extraídos raramente estão prontos para análise imediatamente. A etapa de transformação envolve a aplicação de diversos processos para limpar, filtrar, combinar e formatar os dados de acordo com as necessidades da análise. Transformações também podem incluir cálculos, conversões de formatos, renomeação de colunas e agregações.\n",
        "3.  **<font color=\"#f4a261\"> Carregamento (Load): </font>**  Nesta etapa, os dados transformados são carregados no destino final, como um data warehouse ou um banco de dados de análise. Dependendo do sistema de destino, isso pode envolver a criação de tabelas, esquemas e a definição de chaves primárias ou índices.\n",
        "\n",
        "Ao reunir esses dados de forma organizada e prepará-los para análise, as empresas podem adquirir informações preciosas para direcionar suas decisões e estratégias com mais embasamento.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g2ViU5aYPt2"
      },
      "source": [
        "## Importando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afASEfYAW2uo",
        "outputId": "814d8194-d2a2-438a-e8dd-7765f829183a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/eduardaaraujo/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ZXsw3jZMAo"
      },
      "source": [
        "## Extração de texto de um artigo na web\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5YG-7tjqZRey"
      },
      "outputs": [],
      "source": [
        "# Definindo a classe WebScraper e extraindo texto de um artigo:\n",
        "class WebScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def extract_article_text(self):\n",
        "        # Fazendo uma requisição HTTP GET para a URL fornecida:\n",
        "        response = requests.get(self.url)\n",
        "\n",
        "        # Obtendo o conteúdo HTML:\n",
        "        html_content = response.content\n",
        "\n",
        "        # Criando um objeto BeautifulSoup para analisar o HTML:\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extraindo o texto do artigo:\n",
        "        article_text = soup.get_text()\n",
        "\n",
        "        # Retornando o texto do artigo:\n",
        "        return article_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKfUDmQrZ9xF"
      },
      "source": [
        "O objetivo é armazenar a frequência de cada palavra no artigo. Para isso, é necessário limpar e pré-processar o texto extraído do artigo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSLVI4zia7Vs"
      },
      "source": [
        "## Processador de texto\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3I_9lSrJa-Ms"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, nltk_stopwords):\n",
        "        self.nltk_stopwords = nltk_stopwords\n",
        "\n",
        "    def tokenize_and_clean(self, text):\n",
        "        words = text.split()\n",
        "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
        "        return filtered_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpj2DAuGbI6M"
      },
      "source": [
        "* 'nltk_stopwords': são as stopwords do NLTK (Natural Language Toolkit). Stopwords são palavras comuns, como \"a\", \"an\", \"the\", \"is\", \"and\", etc., que geralmente não carregam muita informação em análises de texto e, portanto, podem ser removidas.\n",
        "\n",
        "\n",
        "* 'tokenize_and_clean': aceita um argumento chamado text, que é o texto que precisa ser processado.\n",
        "\n",
        "\n",
        "\n",
        "> 1.   'words = text.split()': divide o texto em palavras individuais com base nos espaços em branco. Isso cria uma lista de palavras.\n",
        "2.   'filtered_words = [...]': é uma compreensão de lista que itera sobre cada palavra na lista words. O loop verifica duas condições para cada palavra:\n",
        "  *   'word.isalpha()': Verifica se a palavra contém apenas caracteres alfabéticos (letras) e não contém números ou caracteres especiais.\n",
        "  *   'word.lower() not in self.nltk_stopwords':  Verifica se a versão em letras minúsculas da palavra não está presente na lista de stopwords fornecida durante a inicialização da classe.\n",
        "3.   'word.lower()': converte cada palavra para letras minúsculas. Isso ajuda a tratar palavras em maiúsculas e minúsculas como equivalentes.\n",
        "\n",
        "\n",
        "O método devolve uma lista de palavras limpas e processadas, excluindo stopwords e palavras não alfabéticas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gffefPGPcqz8"
      },
      "source": [
        "## Processo ETL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JS3lVSHncUpf"
      },
      "outputs": [],
      "source": [
        "class ETLPipeline:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def run(self):\n",
        "        scraper = WebScraper(self.url)\n",
        "        article_text = scraper.extract_article_text()\n",
        "\n",
        "        processor = TextProcessor(self.nltk_stopwords)\n",
        "        filtered_words = processor.tokenize_and_clean(article_text)\n",
        "\n",
        "        word_freq = Counter(filtered_words)\n",
        "        df = pd.DataFrame(word_freq.items(), columns=[\"Palavras\", \"Frequências\"])\n",
        "        df = df.sort_values(by=\"Frequências\", ascending=False)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtSfpGkXdLUJ"
      },
      "source": [
        "Foi feita a extração de um texto de um artigo da web, processamento para contar a frequência das palavras e retorno dos resultados em um DataFrame ordenado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTzkviVKdf9g"
      },
      "source": [
        "## Execução do Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUUrai1MfE4E"
      },
      "source": [
        "Exemplo 1: Artigo \"The Proven Path to Doing Unique and Meaningful Work\" por James Clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q65LhmP9daIH",
        "outputId": "41bd6960-b681-4d5d-aaea-999d4c49a5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Palavras  Frequências\n",
            "41        bus           25\n",
            "5        work           15\n",
            "80      years            9\n",
            "191    better            7\n",
            "210     ideas            7\n",
            "42    station            7\n",
            "137       get            7\n",
            "0        stay            6\n",
            "40   helsinki            6\n",
            "25        new            6\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    article_url = \"https://jamesclear.com/stay-on-the-bus\"\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()\n",
        "    print(result_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_F70Xbfh5P"
      },
      "source": [
        "Exemplo 2: Artigo \"Athleisure, barre and kale: the tyranny of the ideal woman\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5SOB02ffQTV",
        "outputId": "64e2307f-f913-43ba-fc4f-1ae4ce682def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Palavras  Frequências\n",
            "124        like           21\n",
            "3         woman           19\n",
            "2         ideal           18\n",
            "4         women           16\n",
            "0         barre           14\n",
            "151        even           13\n",
            "214        life           12\n",
            "273        time           12\n",
            "203    exercise           11\n",
            "160         way           11\n",
            "830  athleisure            9\n",
            "101        many            9\n",
            "97         work            9\n",
            "216       feels            8\n",
            "150        look            8\n",
            "102      people            8\n",
            "275      social            7\n",
            "62       always            7\n",
            "48         long            7\n",
            "265          us            7\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    article_url = \"https://www.theguardian.com/news/2019/aug/02/athleisure-barre-kale-tyranny-ideal-woman-labour\"\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()\n",
        "    print(result_df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JggV5uIhNzn"
      },
      "source": [
        "# Conclusão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_8ZjDxhV-y"
      },
      "source": [
        "Um pipeline ETL (Extração, Transformação e Carregamento) de dados da Web é um procedimento organizado e sistemático empregado na engenharia de dados para adquirir, processar e integrar informações vindas de diversas fontes online, as transformando em dados estruturados e acessíveis. Esse processo possibilita a análise eficiente e o armazenamento desses dados, contribuindo para insights valiosos e tomadas de decisão fundamentadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzOyqrBihmKO"
      },
      "source": [
        "## Possíveis casos de uso\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x40baTvwhovn"
      },
      "source": [
        "\n",
        "\n",
        "1.   Monitoramento de Mídias Sociais: Empresas podem coletar e analisar dados de redes sociais para entender sentimentos do público, acompanhar tendências e melhorar estratégias de marketing.\n",
        "\n",
        "2.   Coleta de Dados Financeiros: Para análises de mercado, empresas podem extrair dados financeiros, como preços de ações, taxas de câmbio e indicadores econômicos, de várias fontes online.\n",
        "\n",
        "3.   Raspagem de Dados para Pesquisa: Pesquisadores podem usar pipelines ETL para coletar dados relevantes para suas pesquisas acadêmicas de fontes na web.\n",
        "\n",
        "4.   Acompanhamento de Preços e Produtos: Empresas de e-commerce podem extrair dados de preços e informações de produtos de sites concorrentes para ajustar suas estratégias de preços.\n",
        "\n",
        "5.   Análise de Notícias e Opiniões: Empresas podem coletar e processar notícias, artigos e opiniões da web para entender a percepção do público e tomar decisões informadas.\n",
        "\n",
        "6.   Dados de Sensoriamento Remoto: Para monitorar condições ambientais, dados de sensoriamento remoto de satélites e estações meteorológicas podem ser coletados e transformados.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
